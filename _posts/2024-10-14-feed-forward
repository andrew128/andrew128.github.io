---
layout: post
title: Feed forward Neural Networks
---

What are multi-headed self attention blocks and how do they fit into transformers. 

## Transformer blocks